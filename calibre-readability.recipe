#! /usr/bin/env python


__license__   = 'Postcardware (http://en.wikipedia.org/wiki/Postcardware), contact author for postal address.'
__copyright__ = '2011, Olek Poplavsky <woodenbits at gmail.com>'

from calibre import strftime
from calibre.web.feeds.news import BasicNewsRecipe
import logging
import sys

class Readitlater(BasicNewsRecipe):
    title                 = 'Readability'
    __author__            = 'Olek Poplavsky'
    description           = '''Custom news feeds for Readability. Go to readability.com to
                               setup up your account, then add some articles to your reading list
                               and/or favorites. Fill in your account username and password.'''
    publisher             = 'Woodenbits'
    category              = 'news, custom'
    oldest_article        = 180
    max_articles_per_feed = 200
    no_stylesheets        = True # default False
    use_embedded_content  = False # default None
    needs_subscription    = True # default False
    auto_cleanup = True # default False
#    delay = 0.2 # default 0
    simultaneous_downloads = 5 # set to 1 anyway if delay is > 0
    timeout = 120.0 # in seconds, default 120

    add_metadata           = True # if you want metadata before each article
    archive                = False # automatically archive downloaded articles
    archive_items = []

    INDEX                 = u'https://www.readability.com'

    extra_css = '''
                    h1{font-weight:bold;font-size:x-large;}
                    h2{font-weight:bold;font-size:large;}
                    h3{font-weight:bold;font-size:normal;}
                    h4{font-weight:bold;font-style:italic;font-size:normal;}
                    h5{font-weight:bold;font-style:italic;font-size:normal;color:#333333;}
                '''

    def get_browser(self):
        br = BasicNewsRecipe.get_browser(self)
        br.set_handle_referer(True)
        if self.username is not None and self.password is not None:
            login_url = self.INDEX + u'/login'
            br.open(login_url)
            br.form = list(br.forms())[0]
            br['username'] = self.username
            br['password'] = self.password
            response = br.submit()
        return br

    def get_feeds(self):
        self.report_progress(0, ('Generating list of feeds...'))
        lfeeds = []

        reading_list_url = self.INDEX + u'/reading-list'
        favorites_url = reading_list_url + '/favorites'
        self.report_progress(0, (favorites_url))
        lfeeds.append((u'Reading List', reading_list_url))
        lfeeds.append((u'Favorites', favorites_url))

        return lfeeds

    def parse_index(self):
        totalfeeds = []
        lfeeds = self.get_feeds()
        for feedobj in lfeeds:
            feedtitle, feedurl = feedobj
            self.report_progress(0, ('Fetching feed')+' %s...'%(feedtitle if feedtitle else feedurl))
            articles = []
            soup = self.index_to_soup(feedurl)
            ritem = soup.find('ul',attrs={'class':'bookmark-list'})
            for item in ritem.findAll('li', attrs={'class':'bookmark\n    \n    \n    \n    '}):
                description = item.find('p',attrs={'class':'article-excerpt'})
                processed_article_tag = item.find('h3',attrs={'class':'article-title'})
#                print("...")
#                print(description)
#                print(processed_article_tag)
# using same thing for description and description_tag because it's all I can find
                description_tag = item.find('p',attrs={'class':'article-excerpt'})
#                print(description_tag)
# taking date_tag out because there don't seem to be datestamps any more :(
#								date_tag = article.find('time')
                author_tag = item.find('a',attrs={'class':'article-origin icon icon-favorite'})
 #               print("*")
 #               print(author_tag)
                url         = item.find('a',attrs={'class':'article-origin icon icon-favorite'})['href']
                if not url.startswith("http"):
                		url = self.INDEX + url
 #               print(url)
                title_tag   = item.find('h3',attrs={'class':'article-title'})
                title       = self.tag_to_string(title_tag)
#                date        = self.tag_to_string(date_tag)
                description = self.tag_to_string(description_tag)
                author      = self.tag_to_string(author_tag)
                articles.append({
																	'title'       : title,
#																	'date'        : date,
																	'author'      : author,
																	'url'         : url,
																	'description' : description
																})
            if len(articles) > 0:
                totalfeeds.append((feedtitle, articles))
        if len(totalfeeds) < 1:
            self.abort_recipe_processing("You don't have any articles in your Reading List")
        return totalfeeds

    def preprocess_html(self, soup):
        for original_article_tag in soup.findAll('a', attrs={'id':'article-url'}):
            original_article_url = original_article_tag['href']
            if original_article_url is not None:
                import re
                original_article_url = re.sub(r"^https?://", '', original_article_url)
                del original_article_tag['style']
                original_article_tag.contents[0].replaceWith('Original:&nbsp;&nbsp;' + original_article_url)
                # original_article_tag.replaceWith(original_article_url)
        return soup

    def populate_article_metadata(self, article, soup, first):
        if first:
            # sometimes downloading fails. here we know it was successfully downloaded.
            self.archive_items.append(article.url + '/ajax/archive')

            if self.add_metadata:
                body = soup.find('body')
                if article.summary:
                    body.insert(1,'<h6>Summary:&nbsp;&nbsp;' + article.summary + '</h6>')
                if article.author:
                    body.insert(1,'<h3>Author:&nbsp;&nbsp;' + article.author + '</h3>')
                body.insert(1,'<h2>Title:&nbsp;&nbsp;' + article.title + '</h2>')
                body.insert(1,'<h6>Original:&nbsp;&nbsp;<a href="' + article.url + '>' + article.url +'</a></h6>')

    def cleanup(self):
        if self.archive:
            for item in self.archive_items:
                try:
                    self.report_progress(0, ("Archiving " + item))
                    self.browser.open(item);
                except Exception, e:
                    pass
